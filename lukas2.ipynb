{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from glob import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn \n",
    "from torchvision.ops import DeformConv2d \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pycocotools.coco as coco\n",
    "from pycocotools.cocoeval import COCOeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'data/F0'\n",
    "Images = os.path.join(base, 'Images', '1/*.tiff') \n",
    "Poses = os.path.join(base, 'Poses', '1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [os.path.basename(fn) for fn in sorted(glob(Images))]\n",
    "poses = json.load(open(Poses, 'rb'))['images']\n",
    "\n",
    "images_with_pose = []\n",
    "for pose in poses:  # pose is a dictionary\n",
    "    images_with_pose.append(pose['imagefile'])\n",
    "\n",
    "images_with_pose = sorted(images_with_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# all images are annotated with a pose! (F0 lane 1)\n",
    "images = np.asarray(images)\n",
    "images_with_pose = np.asarray(images_with_pose)\n",
    "(images == images_with_pose).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "passed at: data/F0\\Images\\1\n",
      "passed at: data/F0\\Images\\10\n",
      "passed at: data/F0\\Images\\11\n",
      "passed at: data/F0\\Images\\12\n",
      "passed at: data/F0\\Images\\13\n",
      "passed at: data/F0\\Images\\2\n",
      "passed at: data/F0\\Images\\3\n",
      "passed at: data/F0\\Images\\4\n",
      "passed at: data/F0\\Images\\5\n",
      "passed at: data/F0\\Images\\6\n",
      "passed at: data/F0\\Images\\7\n",
      "passed at: data/F0\\Images\\8\n",
      "passed at: data/F0\\Images\\9\n",
      "passed at: data/F1\\Images\\1\n",
      "passed at: data/F1\\Images\\2\n",
      "passed at: data/F1\\Images\\3\n",
      "passed at: data/F1\\Images\\4\n",
      "passed at: data/F1\\Images\\5\n",
      "passed at: data/F1\\Images\\6\n",
      "passed at: data/F2\\Images\\1\n",
      "passed at: data/F2\\Images\\2\n",
      "passed at: data/F2\\Images\\3\n",
      "passed at: data/F2\\Images\\4\n",
      "passed at: data/F2\\Images\\5\n",
      "passed at: data/F2\\Images\\6\n",
      "passed at: data/F2\\Images\\7\n",
      "passed at: data/F2\\Images\\8\n",
      "passed at: data/F3\\Images\\1\n",
      "passed at: data/F3\\Images\\10\n",
      "passed at: data/F3\\Images\\11\n",
      "passed at: data/F3\\Images\\12\n",
      "passed at: data/F3\\Images\\2\n",
      "passed at: data/F3\\Images\\3\n",
      "passed at: data/F3\\Images\\4\n",
      "passed at: data/F3\\Images\\5\n",
      "passed at: data/F3\\Images\\6\n",
      "passed at: data/F3\\Images\\7\n",
      "passed at: data/F3\\Images\\8\n",
      "passed at: data/F3\\Images\\9\n",
      "passed at: data/F5\\Images\\1\n",
      "passed at: data/F5\\Images\\10\n",
      "passed at: data/F5\\Images\\2\n",
      "passed at: data/F5\\Images\\3\n",
      "passed at: data/F5\\Images\\4\n",
      "passed at: data/F5\\Images\\5\n",
      "passed at: data/F5\\Images\\6\n",
      "passed at: data/F5\\Images\\7\n",
      "passed at: data/F5\\Images\\8\n",
      "passed at: data/F5\\Images\\9\n",
      "passed at: data/F6\\Images\\1\n",
      "passed at: data/F6\\Images\\10\n",
      "passed at: data/F6\\Images\\2\n",
      "passed at: data/F6\\Images\\3\n",
      "passed at: data/F6\\Images\\4\n",
      "passed at: data/F6\\Images\\5\n",
      "passed at: data/F6\\Images\\6\n",
      "passed at: data/F6\\Images\\7\n",
      "passed at: data/F6\\Images\\8\n",
      "passed at: data/F6\\Images\\9\n",
      "passed at: data/F8\\Images\\1\n",
      "passed at: data/F8\\Images\\10\n",
      "passed at: data/F8\\Images\\11\n",
      "passed at: data/F8\\Images\\2\n",
      "passed at: data/F8\\Images\\3\n",
      "passed at: data/F8\\Images\\4\n",
      "passed at: data/F8\\Images\\5\n",
      "passed at: data/F8\\Images\\6\n",
      "passed at: data/F8\\Images\\7\n",
      "passed at: data/F8\\Images\\8\n",
      "passed at: data/F8\\Images\\9\n",
      "passed at: data/F9\\Images\\1\n",
      "passed at: data/F9\\Images\\10\n",
      "passed at: data/F9\\Images\\2\n",
      "passed at: data/F9\\Images\\3\n",
      "passed at: data/F9\\Images\\4\n",
      "passed at: data/F9\\Images\\5\n",
      "passed at: data/F9\\Images\\6\n",
      "passed at: data/F9\\Images\\7\n",
      "passed at: data/F9\\Images\\8\n",
      "passed at: data/F9\\Images\\9\n",
      "passed at: data/F10\\Images\\1\n",
      "passed at: data/F10\\Images\\10\n",
      "passed at: data/F10\\Images\\11\n",
      "passed at: data/F10\\Images\\2\n",
      "passed at: data/F10\\Images\\3\n",
      "passed at: data/F10\\Images\\4\n",
      "passed at: data/F10\\Images\\5\n",
      "passed at: data/F10\\Images\\6\n",
      "passed at: data/F10\\Images\\7\n",
      "passed at: data/F10\\Images\\8\n",
      "passed at: data/F10\\Images\\9\n",
      "passed at: data/F11\\Images\\1\n",
      "passed at: data/F11\\Images\\2\n",
      "passed at: data/F11\\Images\\3\n",
      "passed at: data/F11\\Images\\4\n",
      "passed at: data/F11\\Images\\5\n",
      "passed at: data/F11\\Images\\6\n",
      "passed at: data/F11\\Images\\7\n",
      "passed at: data/F11\\Images\\8\n"
     ]
    }
   ],
   "source": [
    "def poses_info(folder_paths: List[str]):\n",
    "    for folder_path in folder_paths:\n",
    "        for image_folder in glob(os.path.join(folder_path, 'Images', '*')):\n",
    "            folder_nr = image_folder.split('\\\\')[-1]\n",
    "            \n",
    "            Images = os.path.join(image_folder, '*.tiff')\n",
    "            images = [os.path.basename(fn) for fn in sorted(glob(Images))] \n",
    "            \n",
    "            Poses = os.path.join(folder_path, 'Poses', f'{folder_nr}.json')\n",
    "            poses = json.load(open(Poses, 'rb'))['images']\n",
    "\n",
    "            images_with_pose = []\n",
    "            for pose in poses:  # pose is a dictionary\n",
    "                images_with_pose.append(pose['imagefile'])\n",
    "            \n",
    "            images_with_pose = sorted(images_with_pose)\n",
    "            images = np.asarray(images)\n",
    "            images_with_pose = np.asarray(images_with_pose)\n",
    "            if (images == images_with_pose).all():\n",
    "                # all images have a pose\n",
    "                print('passed at:', image_folder)\n",
    "            else:\n",
    "                print('failed at:', image_folder)\n",
    "\n",
    "\n",
    "poses_info([f'data/F{i}' for i in range(12)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(folder_paths: List[str]): \n",
    "    data = {}\n",
    "    nr = 0  # unique lane number, can be used to shuffle in training\n",
    "    for folder_path in folder_paths:\n",
    "        pose_files = sorted(glob(os.path.join(folder_path, 'Poses', '*.json')))\n",
    "        label_files = sorted(glob(os.path.join(folder_path, 'Labels', '*.json')))\n",
    "        for pose_file, label_file in zip(pose_files, label_files):\n",
    "            data[nr] = {} \n",
    "            poses = json.load(open(pose_file, 'rb'))['images']\n",
    "            labels = json.load(open(label_file, 'rb'))['Labels']\n",
    "            \n",
    "            images_with_poses = []\n",
    "            polys = []\n",
    "            for pose in poses:\n",
    "                images_with_poses.append((pose['imagefile'], pose['M3x4']))\n",
    "\n",
    "            if isinstance(labels, list) and len(labels):  \n",
    "                for label in labels:\n",
    "                    polys.append(label['poly'])\n",
    "                # for each lane we have a single annotated image (for the integral image)\n",
    "                data[nr]['annotated_image'] = labels[0]['imagefile']\n",
    "                # polygons: [ [[x1, y1], ...next point], ...next polygon ]\n",
    "                data[nr]['polys'] = polys\n",
    "            else:  # otherwise empty => no annotations (no humans)\n",
    "                data[nr]['annotated_image'] = data[nr]['polys'] = None\n",
    "\n",
    "            # we have the camera extrinsics for each image in the lane\n",
    "            data[nr]['images_with_poses'] = images_with_poses\n",
    "            nr += 1\n",
    "            \n",
    "    return data\n",
    "\n",
    "\n",
    "# training folder names are from F0 to F11\n",
    "data = extract_data([f'data/F{i}' for i in range(12)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-38-f2ea14a1fa11>, line 52)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-38-f2ea14a1fa11>\"\u001b[1;36m, line \u001b[1;32m52\u001b[0m\n\u001b[1;33m    def\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Baseset(Dataset):\n",
    "\n",
    "    def __init__(self, folders: List[str], h: int, w: int, transform=None):\n",
    "        super().__init__()\n",
    "        self.folders = folders  # e.g. ['data/F0', ...]\n",
    "        self.h = h  # image height\n",
    "        self.w = w  # image width\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.data = self.extract_data(folders)\n",
    "        self.size = len(list(self.data.keys()))\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_data(folders: List[str]):\n",
    "        '''\n",
    "        {0: {'images_with_poses': [('name.tiff', pose_M3x4), ...], \n",
    "             'annotated_image': 'center.tiff', \n",
    "             'polys': [[...]], }, \n",
    "         1: {...}, ...}\n",
    "        ''' \n",
    "        data = {}\n",
    "        nr = 0  # unique lane number, can be used to shuffle in training\n",
    "        for folder_path in folders:\n",
    "            pose_files = sorted(glob(os.path.join(folder_path, 'Poses', '*.json')))\n",
    "            label_files = sorted(glob(os.path.join(folder_path, 'Labels', '*.json')))\n",
    "            for pose_file, label_file in zip(pose_files, label_files):\n",
    "                data[nr] = {} \n",
    "                poses = json.load(open(pose_file, 'rb'))['images']\n",
    "                labels = json.load(open(label_file, 'rb'))['Labels']\n",
    "                \n",
    "                images_with_poses = []\n",
    "                polys = []\n",
    "                for pose in poses:\n",
    "                    images_with_poses.append((pose['imagefile'], pose['M3x4']))\n",
    "\n",
    "                if isinstance(labels, list) and len(labels):  \n",
    "                    for label in labels:\n",
    "                        polys.append(label['poly'])\n",
    "                    # each lane has a single annotated image (the integral image)\n",
    "                    data[nr]['annotated_image'] = labels[0]['imagefile']\n",
    "                    # polygons: [ [[x1, y1], ...next point], ...next polygon ]\n",
    "                    data[nr]['polys'] = polys\n",
    "                else:  # otherwise empty => no annotations (no humans)\n",
    "                    data[nr]['annotated_image'] = data[nr]['polys'] = None\n",
    "\n",
    "                # we have the camera extrinsics for each image in the lane\n",
    "                data[nr]['images_with_poses'] = images_with_poses\n",
    "                nr += 1\n",
    "                \n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def load_camera_params(param_folder: str = 'calibration/parameters'):\n",
    "        ''' Camera intrinsics K and distortion coefficients. '''\n",
    "        K = np.load(os.path.join(folder, 'K.npy'))\n",
    "        dist_coeffs = np.load(os.path.join(folder, 'dist_coeffs.npy'))\n",
    "        return K, dist_coeffs\n",
    "\n",
    "    @staticmethod\n",
    "    def imread(paths: List[str], K, dist_coeffs, undistort=False):\n",
    "        images = []\n",
    "        for path in paths:\n",
    "            image = cv2.imread(path, cv2.IMREAD_ANYDEPTH)  # 16 bit image\n",
    "            image = cv2.normalize(image, dst=None, alpha=0, beta=2**16 - 1, \n",
    "                norm_type=cv2.NORM_MINMAX)\n",
    "            image = (image >> 8).astype(np.uint8)  # 8 bit image\n",
    "            \n",
    "            if undistort:  # undistort camera images\n",
    "                h, w = image.shape\n",
    "                # new camera intrinsics based on free scaling parameter\n",
    "                refined_K, roi = cv2.getOptimalNewCameraMatrix(K, dist_coeffs, \n",
    "                    (w, h), 1, (w, h))\n",
    "                x, y, w, h = roi\n",
    "                image = image[y:y+h, x:x+w]\n",
    "                image = cv2.undistort(image, K, dist_coeffs, None, refined_K)\n",
    "            \n",
    "            images.append(image)\n",
    "\n",
    "        return images\n",
    "\n",
    "    @staticmethod\n",
    "    def integrate(images_with_poses: List[Tuple[str, List[List[float]]]], z: float):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def _float(self, x: float):\n",
    "        return float(f'{x:0.2f}')\n",
    "\n",
    "\n",
    "class IntegralDataset(Baseset):\n",
    "    ''' For training and testing of the deep learning based integration. '''\n",
    "\n",
    "    def __init__(self, folders: List[str], h: int, w: int, \n",
    "        sequence_len=5, transform=None):\n",
    "        super().__init__(folders, h, w, transform)\n",
    "        self.sequence_len = sequence_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dict_ = self.data[idx]  # select lane\n",
    "\n",
    "        # randomly select adjacent images\n",
    "\n",
    "\n",
    "        if self.transform is not None:\n",
    "            # data augmentation on images\n",
    "            image_sequence, integral_image = self.transform(image_sequence, integral_image)\n",
    "        \n",
    "        return image_sequence, integral_image\n",
    "\n",
    "\n",
    "class Testset(Baseset):\n",
    "\n",
    "    def __init__(self, folders: List[str], h: int, w: int, transform=None):\n",
    "        super().__init__(folders, h, w, transform)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ]
}